# Document_QA_chatbot_RAG
# ğŸ¤– PDF Chatbot with RAG, MMR Search & Mistral (Ollama)

An intelligent **Conversational AI assistant** that lets you chat with PDF documents. Powered by:
- ğŸ” **MMR-based Retrieval + Reranking**
- ğŸ§  **Mistral-7B (Ollama)** for both Q&A and summarization
- ğŸ§¾ Dynamic **max_tokens allocation**
- ğŸ—ƒï¸ Contextual memory with **LangChainâ€™s ConversationBufferMemory**
- âš™ï¸ Reliable **streaming with error fallback**

---

## ğŸš€ Features

- ğŸ§  **Chat with PDFs** â€“ Ask questions or get summaries from any uploaded document
- ğŸ”„ **MMR Search** â€“ Uses Maximal Marginal Relevance to balance relevance and diversity in retrieval
- ğŸ§  **LangChain Memory** â€“ Keeps track of recent chat context (last 3â€“5 turns)
- ğŸ“ˆ **Dynamic Token Budgeting** â€“ Adjusts output length based on input size (supports large PDFs)
- ğŸ’¬ **Streaming LLM Output** â€“ Real-time answers with error handling fallback
- ğŸ’» **Runs locally** with Ollama (no API keys needed!)

---

## ğŸ§° Tech Stack

| Component       | Technology                                      |
|----------------|--------------------------------------------------|
| LLM            | [`mistral`](https://ollama.com/library/mistral) via [Ollama](https://ollama.com) |
| Embeddings     | `sentence-transformers/all-MiniLM-L6-v2`         |
| Vector Store   | FAISS                                            |
| Tokenizer      | HuggingFace Transformers                         |
| Retrieval      | MMR + Cross-Encoder Reranking                    |
| Memory         | LangChain `ConversationBufferMemory`            |

---


## ğŸ§ª Example Interaction
```markdown
Choose mode - 'qna', 'summary', 'clear', 'show history', 'exit': qna

ğŸ’¬ Ask your question: Whatâ€™s the main objective of the document?

ğŸ¤– The primary objective of the report is to explore the role of AI in streamlining operational workflows across industries...

Choose mode: summary

ğŸ“„ Summary:  
This document discusses the growing influence of AI, emphasizing its applications in predictive analytics, efficiency optimization...

```


## ğŸ“Œ Notes

- Ensure that the **Ollama server is running** in the background before starting the application.
- Open a **separate terminal window** and run the following command:
  
  ```
  ollama run mistral
  ```

- ğŸŸ£ If itâ€™s your first time using the model, this command will download and start the Mistral model locally.  
- ğŸ“˜ Keep this terminal window open while using the app, as it communicates with the running Ollama server.  
- ğŸ’¡ Avoid closing the terminal running Ollama, or the chatbot application will stop responding.
