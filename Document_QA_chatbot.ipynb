{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8685140d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LLM_projects\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sentence_transformers import CrossEncoder\n",
    "import torch #Transformers use torch tensors internally for computation.Even inference (via transformers, sentence-transformers, etc.) may use it.\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac5726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pdf\n",
    "def load_pdf(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Token Counter\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# Dynamic token counter\n",
    "def get_dynamic_max_tokens(task_type: str, prompt_text: str, model_limit: int = 8192) -> int:\n",
    "    input_tokens = count_tokens(prompt_text)\n",
    "    available_tokens = max(200, model_limit - input_tokens)\n",
    "\n",
    "    if task_type == \"qna\":\n",
    "        return min(1024, int(available_tokens * 0.25))\n",
    "    elif task_type == \"summary\":\n",
    "        return min(2048, int(available_tokens * 0.5))\n",
    "    else:\n",
    "        return min(1024, available_tokens)\n",
    "\n",
    "# Text Splitter\n",
    "def split_text(text):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    return splitter.create_documents([text])\n",
    "\n",
    "# Vector Store (FAISS)\n",
    "def create_vector_store(chunks):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "# Load Ollama LLM\n",
    "def get_llm():\n",
    "    return ChatOllama(\n",
    "        model=\"mistral\",\n",
    "        temperature=0.7,\n",
    "        top_k=40,\n",
    "        top_p=0.9,        \n",
    "        streaming=True  \n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f7318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Templates for Q&A\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an intelligent assistant that answers questions based on the context and prior chat history.\n",
    "Don't repeat answers or fabricate information.\n",
    "\n",
    "Answer as accurately and concisely as possible.\n",
    "If the context does not contain the answer, just say: \"I‚Äôm not sure based on the document.\"\n",
    "Only use information from the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8d7f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Templates for Summary\n",
    "summary_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a summarization assistant. Your job is to read the following document and write a clear, concise summary.\n",
    "\n",
    "- Focus on the key arguments, ideas, and important facts.\n",
    "- Keep the summary short and easy to understand.\n",
    "- Do not add anything that is not in the document.\n",
    "\n",
    "Document:\n",
    "{text}\n",
    "\n",
    "Summary:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbe4c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Reranker with Cross-Encoder - Avoiding repeated or too-similar chunks and MMR is one of the method\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "def retrieve_and_rerank(query, vectorstore, threshold=0.7, top_k=5):\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"mmr\", #Maximal Marginal Relevance\n",
    "        search_kwargs={\"k\": top_k * 2, \"lambda_mult\": 0.5}  # Œª = relevance vs diversity\n",
    "    )\n",
    "    mmr_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "    # Filter based on score threshold\n",
    "    scored_docs = vectorstore.similarity_search_with_score(query, k=top_k * 2)\n",
    "    scored_dict = {doc.page_content: score for doc, score in scored_docs}\n",
    "    filtered_docs = [doc for doc in mmr_docs if scored_dict.get(doc.page_content, 0) > threshold]\n",
    "\n",
    "   \n",
    "    if not filtered_docs:\n",
    "        return []\n",
    "\n",
    "    texts = [doc.page_content for doc in filtered_docs]\n",
    "    pairs = [[query, text] for text in texts]\n",
    "    rerank_scores = reranker.predict(pairs)\n",
    "\n",
    "    reranked = sorted(zip(filtered_docs, rerank_scores), key=lambda x: x[1], reverse=True)\n",
    "    return [doc for doc, _ in reranked[:top_k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94cb331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory for storing chat history (used with RAG manually)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Final Pipeline (RAG + LLM + Memory + Chat Display)\n",
    "def run_pipeline_with_memory(docs, vectorstore, llm):\n",
    "    previous_answers = []\n",
    "    chat_history_pairs = []\n",
    "    MAX_HISTORY_TURNS = 5\n",
    "\n",
    "    print(\"‚úÖ Ready! Start chatting below.\")\n",
    "\n",
    "    while True:\n",
    "        mode = input(\"\\nChoose mode - 'qna' for Q&A, 'summary' for Summary, 'clear' to reset memory, 'show history' to display chat history, 'exit' to stop: \").strip().lower()\n",
    "\n",
    "        if mode == \"exit\":\n",
    "            break\n",
    "\n",
    "        elif mode == \"clear\":\n",
    "            memory.clear()\n",
    "            previous_answers.clear()\n",
    "            chat_history_pairs.clear()\n",
    "            clear_output(wait=True)\n",
    "            print(\"üßπ Memory and chat history cleared.\")\n",
    "\n",
    "        elif mode == \"show history\":\n",
    "            if not chat_history_pairs:\n",
    "                print(\"‚ÑπÔ∏è No chat history available yet.\")\n",
    "            else:\n",
    "                print(\"\\nüóÉÔ∏è Chat History:\")\n",
    "                for i, (time, q, a) in enumerate(chat_history_pairs, 1):\n",
    "                    print(f\"{i}. üïí {time}\")\n",
    "                    print(f\"   üßë You: {q}\")\n",
    "                    print(f\"   ü§ñ Bot: {a}\")\n",
    "\n",
    "        elif mode == \"qna\":\n",
    "            query = input(\"\\nüß† Ask your question: \")\n",
    "            relevant_docs = retrieve_and_rerank(query, vectorstore)\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "            inputs = {\n",
    "                \"chat_history\": memory.load_memory_variables({})[\"chat_history\"],\n",
    "                \"context\": context,\n",
    "                \"question\": query\n",
    "            }\n",
    "\n",
    "            message = qa_prompt.format(**inputs)\n",
    "            max_tokens = get_dynamic_max_tokens(\"qna\", message)\n",
    "            print(f\"\\nüïí {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            print(\"üßë You:\", query)\n",
    "            print(\"ü§ñ \", end=\"\", flush=True)\n",
    "\n",
    "            answer = \"\"\n",
    "            try:\n",
    "                for chunk in llm.stream([HumanMessage(content=message)], max_tokens=max_tokens):\n",
    "                    print(chunk.content, end=\"\", flush=True)\n",
    "                    answer += chunk.content\n",
    "            except Exception as e:\n",
    "                print(\"\\n‚ö†Ô∏è Streaming failed, falling back...\\n\")\n",
    "                try:\n",
    "                    answer = llm.invoke([HumanMessage(content=message)], max_tokens=max_tokens).content\n",
    "                    print(answer)\n",
    "                except Exception as fallback_error:\n",
    "                    print(f\"‚ùå LLM failed: {fallback_error}\")\n",
    "                    continue  # Skip this round\n",
    "\n",
    "            print()          \n",
    "\n",
    "            if answer in previous_answers:\n",
    "                print(\"ü§ñ I've already answered this or something very similar. Try a different question.\")\n",
    "            else:\n",
    "                previous_answers.append(answer)\n",
    "                memory.save_context({\"question\": query}, {\"answer\": answer})\n",
    "                chat_history_pairs.append((datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), query, answer))\n",
    "\n",
    "                # Keep only latest MAX_HISTORY_TURNS\n",
    "                if len(chat_history_pairs) > MAX_HISTORY_TURNS:\n",
    "                    chat_history_pairs = chat_history_pairs[-MAX_HISTORY_TURNS:]\n",
    "\n",
    "        elif mode == \"summary\":\n",
    "            docs_for_summary = retrieve_and_rerank(\"summarize the document\", vectorstore)\n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in docs_for_summary])\n",
    "\n",
    "            prompt = summary_prompt.format(text=context)\n",
    "            max_tokens = get_dynamic_max_tokens(\"summary\", prompt)\n",
    "\n",
    "            print(f\"\\nüïí {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            print(\"üìù Summary:\\n\")\n",
    "\n",
    "            summary = \"\"\n",
    "            try:\n",
    "                for chunk in llm.stream([HumanMessage(content=prompt)], max_tokens=max_tokens):\n",
    "                    print(chunk.content, end=\"\", flush=True)\n",
    "                    summary += chunk.content\n",
    "            except Exception as e:\n",
    "                print(\"\\n‚ö†Ô∏è Streaming failed, falling back...\\n\")\n",
    "                try:\n",
    "                    summary = llm.invoke([HumanMessage(content=prompt)], max_tokens=max_tokens).content\n",
    "                    print(summary)\n",
    "                except Exception as fallback_error:\n",
    "                    print(f\"‚ùå LLM failed: {fallback_error}\")\n",
    "                    continue  # Skip this round\n",
    "\n",
    "            print()\n",
    "\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Invalid mode. Please type 'qna', 'summary', 'show history', 'clear', or 'exit'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69513ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Memory and chat history cleared.\n",
      "‚ÑπÔ∏è No chat history available yet.\n",
      "\n",
      "üïí 2025-07-28 01:16:17\n",
      "üßë You: who is lila?\n",
      "ü§ñ  Lila is a 13-year-old girl who discovers an old, time-traveling notebook in her grandmother's attic. She uses the notebook to travel through different points in history and learn from historical events.\n",
      "‚ö†Ô∏è Invalid mode. Please type 'qna', 'summary', 'show history', 'clear', or 'exit'.\n",
      "\n",
      "üïí 2025-07-28 01:18:04\n",
      "üìù Summary:\n",
      "\n",
      " A 13-year-old girl named Lila finds a magical time-traveling notebook in her grandmother's attic. The notebook transports her to various historical periods based on what she writes within it, such as medieval times, ancient Egypt, and the future year 3025. She learns that emotional connection to the event is crucial for successful time travel. During one instance, Lila gets stuck in 1942 and solves a historical mystery to return. Her experience teaches her valuable lessons about courage and trust. Upon returning to Eldridge, she decides to use the notebook to help others learn history by experiencing it themselves.\n"
     ]
    }
   ],
   "source": [
    "# Run full pipeline\n",
    "pdf_path = input(\"Enter path to your PDF (e.g. docs/your_file.pdf): \")\n",
    "if not os.path.exists(pdf_path):\n",
    "    raise FileNotFoundError(\"‚ùå PDF not found.\")\n",
    "else:\n",
    "    print(\"‚è≥ Loading and processing document...\")\n",
    "    text = load_pdf(pdf_path)\n",
    "    total_tokens = count_tokens(text)\n",
    "    print(f\"üìè Total tokens in PDF: {total_tokens}\")\n",
    "    if total_tokens > 8000:\n",
    "        print(\"‚ö†Ô∏è Warning: This exceeds the 8192 token limit of Mistral.\")\n",
    "        print(\"üîÅ Only top relevant chunks will be used during Q&A or summary to stay within limits.\")\n",
    "        \n",
    "    chunks = split_text(text)\n",
    "    docs = [Document(page_content=chunk.page_content) for chunk in chunks]\n",
    "\n",
    "    print(\"üì¶ Creating vector store...\")\n",
    "    vectorstore = create_vector_store(docs)\n",
    "\n",
    "    print(\"üöÄ Loading LLM...\")\n",
    "    llm = get_llm()\n",
    "\n",
    "    run_pipeline_with_memory(docs, vectorstore, llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f4c27f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc56c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
